{"cells":[{"source":"# Data Manipulation with pandas\nRun the hidden code cell below to import the data used in this course.","metadata":{"id":"bA5ajAmk7XH6"},"id":"prostate-arizona","cell_type":"markdown"},{"source":"# Import the course packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Import the four datasets\navocado = pd.read_csv(\"datasets/avocado.csv\")\nhomelessness = pd.read_csv(\"datasets/homelessness.csv\")\ntemperatures = pd.read_csv(\"datasets/temperatures.csv\")\nwalmart = pd.read_csv(\"datasets/walmart.csv\")","metadata":{"scrolled":true,"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"id":"2e25fdd8-4d84-45bc-80f0-949917e00a17","cell_type":"code","execution_count":null,"outputs":[]},{"source":"**CHAPTER 1: Introduction to DataFrames**\n\nPandas is a Python package for data manipulation, it can also be used for data visualization.\nIt is built on the NumPy and Matplotlib package.\n\nMethods in Pandas\n1. head() - Displays the top 5 rows of the DataFrame.\n2. info() - Names of columns, datatypes, and if there are any missing values.\n3. describe() - Summary ststistics.\n\nAttributes - No parenthesis ()\nshape - Returns number of rows and columns.\n\nDataframes consists of 3 components stored as attributes namely:\n1. columns\n2. values\n3. index","metadata":{},"id":"6fad679d","cell_type":"markdown"},{"source":"_Add your notes here_","metadata":{},"id":"e9a448e0","cell_type":"markdown"},{"source":"import pandas as pd\n\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Print the head of the homelessness data\nprint(homelessness.head())\n\n# Print information about homelessness\nprint(homelessness.info())\n\n# Print the shape of homelessness\nprint(homelessness.shape)\n\n# Print a description of homelessness\nprint(homelessness.describe())","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1705417492923,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\n\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Print the head of the homelessness data\nprint(homelessness.head())\n\n# Print information about homelessness\nprint(homelessness.info())\n\n# Print the shape of homelessness\nprint(homelessness.shape)\n\n# Print a description of homelessness\nprint(homelessness.describe())","outputsMetadata":{"0":{"height":597,"type":"stream"}}},"id":"893055c9","cell_type":"code","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"               region       state  individuals  family_members  state_pop\n0  East South Central     Alabama       2570.0           864.0    4887681\n1             Pacific      Alaska       1434.0           582.0     735139\n2            Mountain     Arizona       7259.0          2606.0    7158024\n3  West South Central    Arkansas       2280.0           432.0    3009733\n4             Pacific  California     109008.0         20964.0   39461588\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 51 entries, 0 to 50\nData columns (total 5 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   region          51 non-null     object \n 1   state           51 non-null     object \n 2   individuals     51 non-null     float64\n 3   family_members  51 non-null     float64\n 4   state_pop       51 non-null     int64  \ndtypes: float64(2), int64(1), object(2)\nmemory usage: 2.1+ KB\nNone\n(51, 5)\n         individuals  family_members     state_pop\ncount      51.000000       51.000000  5.100000e+01\nmean     7225.784314     3504.882353  6.405637e+06\nstd     15991.025083     7805.411811  7.327258e+06\nmin       434.000000       75.000000  5.776010e+05\n25%      1446.500000      592.000000  1.777414e+06\n50%      3082.000000     1482.000000  4.461153e+06\n75%      6781.500000     3196.000000  7.340946e+06\nmax    109008.000000    52070.000000  3.946159e+07\n"}]},{"source":"# Import pandas using the alias pd\nimport pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Print the values of homelessness\nprint(homelessness.values)\n\n# Print the column index of homelessness\nprint(homelessness.columns)\n\n# Print the row index of homelessness\nprint(homelessness.index)\n\nhomelessness.sort_values(\"state_pop\")","metadata":{"executionCancelledAt":null,"executionTime":55,"lastExecutedAt":1705418162300,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import pandas using the alias pd\nimport pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Print the values of homelessness\nprint(homelessness.values)\n\n# Print the column index of homelessness\nprint(homelessness.columns)\n\n# Print the row index of homelessness\nprint(homelessness.index)\n\nhomelessness.sort_values(\"state_pop\")","outputsMetadata":{"0":{"height":616,"type":"stream"},"1":{"height":320,"type":"dataFrame"}}},"cell_type":"code","id":"e4b2573d-16d8-48d2-b34d-ba7000e4501b","execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":"[['East South Central' 'Alabama' 2570.0 864.0 4887681]\n ['Pacific' 'Alaska' 1434.0 582.0 735139]\n ['Mountain' 'Arizona' 7259.0 2606.0 7158024]\n ['West South Central' 'Arkansas' 2280.0 432.0 3009733]\n ['Pacific' 'California' 109008.0 20964.0 39461588]\n ['Mountain' 'Colorado' 7607.0 3250.0 5691287]\n ['New England' 'Connecticut' 2280.0 1696.0 3571520]\n ['South Atlantic' 'Delaware' 708.0 374.0 965479]\n ['South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n ['South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n ['South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n ['Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n ['Mountain' 'Idaho' 1297.0 715.0 1750536]\n ['East North Central' 'Illinois' 6752.0 3891.0 12723071]\n ['East North Central' 'Indiana' 3776.0 1482.0 6695497]\n ['West North Central' 'Iowa' 1711.0 1038.0 3148618]\n ['West North Central' 'Kansas' 1443.0 773.0 2911359]\n ['East South Central' 'Kentucky' 2735.0 953.0 4461153]\n ['West South Central' 'Louisiana' 2540.0 519.0 4659690]\n ['New England' 'Maine' 1450.0 1066.0 1339057]\n ['South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n ['New England' 'Massachusetts' 6811.0 13257.0 6882635]\n ['East North Central' 'Michigan' 5209.0 3142.0 9984072]\n ['West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n ['East South Central' 'Mississippi' 1024.0 328.0 2981020]\n ['West North Central' 'Missouri' 3776.0 2107.0 6121623]\n ['Mountain' 'Montana' 983.0 422.0 1060665]\n ['West North Central' 'Nebraska' 1745.0 676.0 1925614]\n ['Mountain' 'Nevada' 7058.0 486.0 3027341]\n ['New England' 'New Hampshire' 835.0 615.0 1353465]\n ['Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n ['Mountain' 'New Mexico' 1949.0 602.0 2092741]\n ['Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n ['South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n ['West North Central' 'North Dakota' 467.0 75.0 758080]\n ['East North Central' 'Ohio' 6929.0 3320.0 11676341]\n ['West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n ['Pacific' 'Oregon' 11139.0 3337.0 4181886]\n ['Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n ['New England' 'Rhode Island' 747.0 354.0 1058287]\n ['South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n ['West North Central' 'South Dakota' 836.0 323.0 878698]\n ['East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n ['West South Central' 'Texas' 19199.0 6111.0 28628666]\n ['Mountain' 'Utah' 1904.0 972.0 3153550]\n ['New England' 'Vermont' 780.0 511.0 624358]\n ['South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n ['Pacific' 'Washington' 16424.0 5880.0 7523869]\n ['South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n ['East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n ['Mountain' 'Wyoming' 434.0 205.0 577601]]\nIndex(['region', 'state', 'individuals', 'family_members', 'state_pop'], dtype='object')\nRangeIndex(start=0, stop=51, step=1)\n"},{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"region","type":"string"},{"name":"state","type":"string"},{"name":"individuals","type":"number"},{"name":"family_members","type":"number"},{"name":"state_pop","type":"integer"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[50,45,8,1,34,41,7,39,26,19,29,11,12,48,27,31,16,24,3,28,15,44,6,36,37,17,18,0,40,23,5,49,20,25,14,42,21,2,47,46,30,22,33,10,35,13,38,32,9,43,4],"region":["Mountain","New England","South Atlantic","Pacific","West North Central","West North Central","South Atlantic","New England","Mountain","New England","New England","Pacific","Mountain","South Atlantic","West North Central","Mountain","West North Central","East South Central","West South Central","Mountain","West North Central","Mountain","New England","West South Central","Pacific","East South Central","West South Central","East South Central","South Atlantic","West North Central","Mountain","East North Central","South Atlantic","West North Central","East North Central","East South Central","New England","Mountain","Pacific","South Atlantic","Mid-Atlantic","East North Central","South Atlantic","South Atlantic","East North Central","East North Central","Mid-Atlantic","Mid-Atlantic","South Atlantic","West South Central","Pacific"],"state":["Wyoming","Vermont","District of Columbia","Alaska","North Dakota","South Dakota","Delaware","Rhode Island","Montana","Maine","New Hampshire","Hawaii","Idaho","West Virginia","Nebraska","New Mexico","Kansas","Mississippi","Arkansas","Nevada","Iowa","Utah","Connecticut","Oklahoma","Oregon","Kentucky","Louisiana","Alabama","South Carolina","Minnesota","Colorado","Wisconsin","Maryland","Missouri","Indiana","Tennessee","Massachusetts","Arizona","Washington","Virginia","New Jersey","Michigan","North Carolina","Georgia","Ohio","Illinois","Pennsylvania","New York","Florida","Texas","California"],"individuals":[434,780,3770,1434,467,836,708,747,983,1450,835,4131,1297,1021,1745,1949,1443,1024,2280,7058,1711,1904,2280,2823,11139,2735,2540,2570,3082,3993,7607,2740,4914,3776,3776,6139,6811,7259,16424,3928,6048,5209,6451,6943,6929,6752,8163,39827,21443,19199,109008],"family_members":[205,511,3134,582,75,323,374,354,422,1066,615,2399,715,222,676,602,773,328,432,486,1038,972,1696,1048,3337,953,519,864,851,3250,3250,2167,2230,2107,1482,1744,13257,2606,5880,2047,3350,3142,2817,2556,3320,3891,5349,52070,9587,6111,20964],"state_pop":[577601,624358,701547,735139,758080,878698,965479,1058287,1060665,1339057,1353465,1420593,1750536,1804291,1925614,2092741,2911359,2981020,3009733,3027341,3148618,3153550,3571520,3940235,4181886,4461153,4659690,4887681,5084156,5606249,5691287,5807406,6035802,6121623,6695497,6771631,6882635,7158024,7523869,8501286,8886025,9984072,10381615,10511131,11676341,12723071,12800922,19530351,21244317,28628666,39461588]}},"total_rows":51,"truncation_type":null},"text/plain":"                region                 state  ...  family_members  state_pop\n50            Mountain               Wyoming  ...           205.0     577601\n45         New England               Vermont  ...           511.0     624358\n8       South Atlantic  District of Columbia  ...          3134.0     701547\n1              Pacific                Alaska  ...           582.0     735139\n34  West North Central          North Dakota  ...            75.0     758080\n41  West North Central          South Dakota  ...           323.0     878698\n7       South Atlantic              Delaware  ...           374.0     965479\n39         New England          Rhode Island  ...           354.0    1058287\n26            Mountain               Montana  ...           422.0    1060665\n19         New England                 Maine  ...          1066.0    1339057\n29         New England         New Hampshire  ...           615.0    1353465\n11             Pacific                Hawaii  ...          2399.0    1420593\n12            Mountain                 Idaho  ...           715.0    1750536\n48      South Atlantic         West Virginia  ...           222.0    1804291\n27  West North Central              Nebraska  ...           676.0    1925614\n31            Mountain            New Mexico  ...           602.0    2092741\n16  West North Central                Kansas  ...           773.0    2911359\n24  East South Central           Mississippi  ...           328.0    2981020\n3   West South Central              Arkansas  ...           432.0    3009733\n28            Mountain                Nevada  ...           486.0    3027341\n15  West North Central                  Iowa  ...          1038.0    3148618\n44            Mountain                  Utah  ...           972.0    3153550\n6          New England           Connecticut  ...          1696.0    3571520\n36  West South Central              Oklahoma  ...          1048.0    3940235\n37             Pacific                Oregon  ...          3337.0    4181886\n17  East South Central              Kentucky  ...           953.0    4461153\n18  West South Central             Louisiana  ...           519.0    4659690\n0   East South Central               Alabama  ...           864.0    4887681\n40      South Atlantic        South Carolina  ...           851.0    5084156\n23  West North Central             Minnesota  ...          3250.0    5606249\n5             Mountain              Colorado  ...          3250.0    5691287\n49  East North Central             Wisconsin  ...          2167.0    5807406\n20      South Atlantic              Maryland  ...          2230.0    6035802\n25  West North Central              Missouri  ...          2107.0    6121623\n14  East North Central               Indiana  ...          1482.0    6695497\n42  East South Central             Tennessee  ...          1744.0    6771631\n21         New England         Massachusetts  ...         13257.0    6882635\n2             Mountain               Arizona  ...          2606.0    7158024\n47             Pacific            Washington  ...          5880.0    7523869\n46      South Atlantic              Virginia  ...          2047.0    8501286\n30        Mid-Atlantic            New Jersey  ...          3350.0    8886025\n22  East North Central              Michigan  ...          3142.0    9984072\n33      South Atlantic        North Carolina  ...          2817.0   10381615\n10      South Atlantic               Georgia  ...          2556.0   10511131\n35  East North Central                  Ohio  ...          3320.0   11676341\n13  East North Central              Illinois  ...          3891.0   12723071\n38        Mid-Atlantic          Pennsylvania  ...          5349.0   12800922\n32        Mid-Atlantic              New York  ...         52070.0   19530351\n9       South Atlantic               Florida  ...          9587.0   21244317\n43  West South Central                 Texas  ...          6111.0   28628666\n4              Pacific            California  ...         20964.0   39461588\n\n[51 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>region</th>\n      <th>state</th>\n      <th>individuals</th>\n      <th>family_members</th>\n      <th>state_pop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>50</th>\n      <td>Mountain</td>\n      <td>Wyoming</td>\n      <td>434.0</td>\n      <td>205.0</td>\n      <td>577601</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>New England</td>\n      <td>Vermont</td>\n      <td>780.0</td>\n      <td>511.0</td>\n      <td>624358</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>South Atlantic</td>\n      <td>District of Columbia</td>\n      <td>3770.0</td>\n      <td>3134.0</td>\n      <td>701547</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Pacific</td>\n      <td>Alaska</td>\n      <td>1434.0</td>\n      <td>582.0</td>\n      <td>735139</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>West North Central</td>\n      <td>North Dakota</td>\n      <td>467.0</td>\n      <td>75.0</td>\n      <td>758080</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>West North Central</td>\n      <td>South Dakota</td>\n      <td>836.0</td>\n      <td>323.0</td>\n      <td>878698</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>South Atlantic</td>\n      <td>Delaware</td>\n      <td>708.0</td>\n      <td>374.0</td>\n      <td>965479</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>New England</td>\n      <td>Rhode Island</td>\n      <td>747.0</td>\n      <td>354.0</td>\n      <td>1058287</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Mountain</td>\n      <td>Montana</td>\n      <td>983.0</td>\n      <td>422.0</td>\n      <td>1060665</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>New England</td>\n      <td>Maine</td>\n      <td>1450.0</td>\n      <td>1066.0</td>\n      <td>1339057</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>New England</td>\n      <td>New Hampshire</td>\n      <td>835.0</td>\n      <td>615.0</td>\n      <td>1353465</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Pacific</td>\n      <td>Hawaii</td>\n      <td>4131.0</td>\n      <td>2399.0</td>\n      <td>1420593</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Mountain</td>\n      <td>Idaho</td>\n      <td>1297.0</td>\n      <td>715.0</td>\n      <td>1750536</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>South Atlantic</td>\n      <td>West Virginia</td>\n      <td>1021.0</td>\n      <td>222.0</td>\n      <td>1804291</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>West North Central</td>\n      <td>Nebraska</td>\n      <td>1745.0</td>\n      <td>676.0</td>\n      <td>1925614</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Mountain</td>\n      <td>New Mexico</td>\n      <td>1949.0</td>\n      <td>602.0</td>\n      <td>2092741</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>West North Central</td>\n      <td>Kansas</td>\n      <td>1443.0</td>\n      <td>773.0</td>\n      <td>2911359</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>East South Central</td>\n      <td>Mississippi</td>\n      <td>1024.0</td>\n      <td>328.0</td>\n      <td>2981020</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>West South Central</td>\n      <td>Arkansas</td>\n      <td>2280.0</td>\n      <td>432.0</td>\n      <td>3009733</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Mountain</td>\n      <td>Nevada</td>\n      <td>7058.0</td>\n      <td>486.0</td>\n      <td>3027341</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>West North Central</td>\n      <td>Iowa</td>\n      <td>1711.0</td>\n      <td>1038.0</td>\n      <td>3148618</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Mountain</td>\n      <td>Utah</td>\n      <td>1904.0</td>\n      <td>972.0</td>\n      <td>3153550</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>New England</td>\n      <td>Connecticut</td>\n      <td>2280.0</td>\n      <td>1696.0</td>\n      <td>3571520</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>West South Central</td>\n      <td>Oklahoma</td>\n      <td>2823.0</td>\n      <td>1048.0</td>\n      <td>3940235</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Pacific</td>\n      <td>Oregon</td>\n      <td>11139.0</td>\n      <td>3337.0</td>\n      <td>4181886</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>East South Central</td>\n      <td>Kentucky</td>\n      <td>2735.0</td>\n      <td>953.0</td>\n      <td>4461153</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>West South Central</td>\n      <td>Louisiana</td>\n      <td>2540.0</td>\n      <td>519.0</td>\n      <td>4659690</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>East South Central</td>\n      <td>Alabama</td>\n      <td>2570.0</td>\n      <td>864.0</td>\n      <td>4887681</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>South Atlantic</td>\n      <td>South Carolina</td>\n      <td>3082.0</td>\n      <td>851.0</td>\n      <td>5084156</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>West North Central</td>\n      <td>Minnesota</td>\n      <td>3993.0</td>\n      <td>3250.0</td>\n      <td>5606249</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Mountain</td>\n      <td>Colorado</td>\n      <td>7607.0</td>\n      <td>3250.0</td>\n      <td>5691287</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>East North Central</td>\n      <td>Wisconsin</td>\n      <td>2740.0</td>\n      <td>2167.0</td>\n      <td>5807406</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>South Atlantic</td>\n      <td>Maryland</td>\n      <td>4914.0</td>\n      <td>2230.0</td>\n      <td>6035802</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>West North Central</td>\n      <td>Missouri</td>\n      <td>3776.0</td>\n      <td>2107.0</td>\n      <td>6121623</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>East North Central</td>\n      <td>Indiana</td>\n      <td>3776.0</td>\n      <td>1482.0</td>\n      <td>6695497</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>East South Central</td>\n      <td>Tennessee</td>\n      <td>6139.0</td>\n      <td>1744.0</td>\n      <td>6771631</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>New England</td>\n      <td>Massachusetts</td>\n      <td>6811.0</td>\n      <td>13257.0</td>\n      <td>6882635</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Mountain</td>\n      <td>Arizona</td>\n      <td>7259.0</td>\n      <td>2606.0</td>\n      <td>7158024</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>Pacific</td>\n      <td>Washington</td>\n      <td>16424.0</td>\n      <td>5880.0</td>\n      <td>7523869</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>South Atlantic</td>\n      <td>Virginia</td>\n      <td>3928.0</td>\n      <td>2047.0</td>\n      <td>8501286</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Mid-Atlantic</td>\n      <td>New Jersey</td>\n      <td>6048.0</td>\n      <td>3350.0</td>\n      <td>8886025</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>East North Central</td>\n      <td>Michigan</td>\n      <td>5209.0</td>\n      <td>3142.0</td>\n      <td>9984072</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>South Atlantic</td>\n      <td>North Carolina</td>\n      <td>6451.0</td>\n      <td>2817.0</td>\n      <td>10381615</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>South Atlantic</td>\n      <td>Georgia</td>\n      <td>6943.0</td>\n      <td>2556.0</td>\n      <td>10511131</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>East North Central</td>\n      <td>Ohio</td>\n      <td>6929.0</td>\n      <td>3320.0</td>\n      <td>11676341</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>East North Central</td>\n      <td>Illinois</td>\n      <td>6752.0</td>\n      <td>3891.0</td>\n      <td>12723071</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Mid-Atlantic</td>\n      <td>Pennsylvania</td>\n      <td>8163.0</td>\n      <td>5349.0</td>\n      <td>12800922</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Mid-Atlantic</td>\n      <td>New York</td>\n      <td>39827.0</td>\n      <td>52070.0</td>\n      <td>19530351</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>South Atlantic</td>\n      <td>Florida</td>\n      <td>21443.0</td>\n      <td>9587.0</td>\n      <td>21244317</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>West South Central</td>\n      <td>Texas</td>\n      <td>19199.0</td>\n      <td>6111.0</td>\n      <td>28628666</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Pacific</td>\n      <td>California</td>\n      <td>109008.0</td>\n      <td>20964.0</td>\n      <td>39461588</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{},"execution_count":5}]},{"source":"**Sorting and Subsetting**\n\nSorting: Sort rows using the sort_values() method, passing the column_name you want to sort by in the parenthesis.\n1. df.sort_values('column_name') - This sorts in ascending order (smallest to largest)\n2. df.sort_values('column_name', ascending = False) - This sorts in descending order.\n3. df.sort_values(['column_name1', 'column_name2']) - This sorts by multiple values.\n4. df.sort_values(['column_name1', 'column_name2'], ascending = [True, False]) - This sorts by multiple values.\n\nSubsetting Columns: \n1. df['column_name'] - Gives result in series.\n2. df[['column_name1', 'column_name2']] or set = ['column_name1', 'column_name2'], df[set]\n\nSubsetting Rows:\n1. df['column_name'] > 25 - Returns boolean values of true and false.\n2. df[df['column_name'] > 25] - Returns row values where true.\n3. Subset based on strings using ==\n4. Subset based on dates, compare a column to date values \"yyyy-mm-dd\"\n5. Using isin() method.","metadata":{},"cell_type":"markdown","id":"755ec5cd-8b5b-47ed-8557-c2ab68cd5ca4"},{"source":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Sort homelessness by individuals\nhomelessness_ind = homelessness.sort_values('individuals')\n\n# Print the top few rows\nprint(homelessness_ind.head())\n\n# Sort homelessness by descending family members\nhomelessness_fam = homelessness.sort_values('family_members', ascending = False)\n\n# Print the top few rows\nprint(homelessness_fam.head())\n\n# Sort homelessness by region, then descending family members\nhomelessness_reg_fam = homelessness.sort_values(['region','family_members'], ascending = [True, False])\n\n# Print the top few rows\nprint(homelessness_reg_fam.head())","metadata":{"executionCancelledAt":null,"executionTime":34,"lastExecutedAt":1705677878019,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Sort homelessness by individuals\nhomelessness_ind = homelessness.sort_values('individuals')\n\n# Print the top few rows\nprint(homelessness_ind.head())\n\n# Sort homelessness by descending family members\nhomelessness_fam = homelessness.sort_values('family_members', ascending = False)\n\n# Print the top few rows\nprint(homelessness_fam.head())\n\n# Sort homelessness by region, then descending family members\nhomelessness_reg_fam = homelessness.sort_values(['region','family_members'], ascending = [True, False])\n\n# Print the top few rows\nprint(homelessness_reg_fam.head())","outputsMetadata":{"0":{"height":377,"type":"stream"}}},"cell_type":"code","id":"e7df3568-9474-443a-810d-4ca368d535dc","outputs":[{"output_type":"stream","name":"stdout","text":"                region         state  individuals  family_members  state_pop\n50            Mountain       Wyoming        434.0           205.0     577601\n34  West North Central  North Dakota        467.0            75.0     758080\n7       South Atlantic      Delaware        708.0           374.0     965479\n39         New England  Rhode Island        747.0           354.0    1058287\n45         New England       Vermont        780.0           511.0     624358\n                region          state  individuals  family_members  state_pop\n32        Mid-Atlantic       New York      39827.0         52070.0   19530351\n4              Pacific     California     109008.0         20964.0   39461588\n21         New England  Massachusetts       6811.0         13257.0    6882635\n9       South Atlantic        Florida      21443.0          9587.0   21244317\n43  West South Central          Texas      19199.0          6111.0   28628666\n                region      state  individuals  family_members  state_pop\n13  East North Central   Illinois       6752.0          3891.0   12723071\n35  East North Central       Ohio       6929.0          3320.0   11676341\n22  East North Central   Michigan       5209.0          3142.0    9984072\n49  East North Central  Wisconsin       2740.0          2167.0    5807406\n14  East North Central    Indiana       3776.0          1482.0    6695497\n"}],"execution_count":4},{"source":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Select the individuals column\nindividuals = homelessness[['individuals']]\n\n# Print the head of the result\nprint(individuals.head())\n\n# Select the state and family_members columns\nstate_fam = homelessness[['state', 'family_members']]\n\n# Print the head of the result\nprint(state_fam.head())\n\n# Select only the individuals and state columns, in that order\nind_state = homelessness[['individuals', 'state']]\n\n# Print the head of the result\nprint(ind_state.head())","metadata":{"executionCancelledAt":null,"executionTime":26,"lastExecutedAt":1705678157588,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Select the individuals column\nindividuals = homelessness[['individuals']]\n\n# Print the head of the result\nprint(individuals.head())\n\n# Select the state and family_members columns\nstate_fam = homelessness[['state', 'family_members']]\n\n# Print the head of the result\nprint(state_fam.head())\n\n# Select only the individuals and state columns, in that order\nind_state = homelessness[['individuals', 'state']]\n\n# Print the head of the result\nprint(ind_state.head())","outputsMetadata":{"0":{"height":377,"type":"stream"}}},"cell_type":"code","id":"f8883e44-13f2-44c1-9cf9-3765241bf2ba","outputs":[{"output_type":"stream","name":"stdout","text":"   individuals\n0       2570.0\n1       1434.0\n2       7259.0\n3       2280.0\n4     109008.0\n        state  family_members\n0     Alabama           864.0\n1      Alaska           582.0\n2     Arizona          2606.0\n3    Arkansas           432.0\n4  California         20964.0\n   individuals       state\n0       2570.0     Alabama\n1       1434.0      Alaska\n2       7259.0     Arizona\n3       2280.0    Arkansas\n4     109008.0  California\n"}],"execution_count":6},{"source":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Filter for rows where individuals is greater than 10000\nind_gt_10k = homelessness[homelessness['individuals'] > 10000]\n\n# See the result\nprint(ind_gt_10k)\n\n# Filter for rows where region is Mountain\nmountain_reg = homelessness[homelessness['region'] == 'Mountain']\n\n# See the result\nprint(mountain_reg)\n\n# Filter for rows where family_members is less than 1000 \n# and region is Pacific\nfam_lt_1k_pac = homelessness[(homelessness['family_members'] < 1000) & (homelessness['region'] == 'Pacific')]\n\n# See the result\nprint(fam_lt_1k_pac)\n\n# Subset for rows in South Atlantic or Mid-Atlantic regions\nsouth_mid_atlantic = homelessness[homelessness['region'].isin(['South Atlantic', 'Mid-Atlantic'])]\n\n# See the result\nprint(south_mid_atlantic)\n\n# The Mojave Desert states\ncanu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n\n# Filter for rows in the Mojave Desert states\nmojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n\n# See the result\nprint(mojave_homelessness)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1705679465745,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Filter for rows where individuals is greater than 10000\nind_gt_10k = homelessness[homelessness['individuals'] > 10000]\n\n# See the result\nprint(ind_gt_10k)\n\n# Filter for rows where region is Mountain\nmountain_reg = homelessness[homelessness['region'] == 'Mountain']\n\n# See the result\nprint(mountain_reg)\n\n# Filter for rows where family_members is less than 1000 \n# and region is Pacific\nfam_lt_1k_pac = homelessness[(homelessness['family_members'] < 1000) & (homelessness['region'] == 'Pacific')]\n\n# See the result\nprint(fam_lt_1k_pac)\n\n# Subset for rows in South Atlantic or Mid-Atlantic regions\nsouth_mid_atlantic = homelessness[homelessness['region'].isin(['South Atlantic', 'Mid-Atlantic'])]\n\n# See the result\nprint(south_mid_atlantic)\n\n# The Mojave Desert states\ncanu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n\n# Filter for rows in the Mojave Desert states\nmojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n\n# See the result\nprint(mojave_homelessness)","outputsMetadata":{"0":{"height":616,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"1c814c3b-0324-48bf-83b3-7a0c50399542","outputs":[{"output_type":"stream","name":"stdout","text":"                region       state  individuals  family_members  state_pop\n4              Pacific  California     109008.0         20964.0   39461588\n9       South Atlantic     Florida      21443.0          9587.0   21244317\n32        Mid-Atlantic    New York      39827.0         52070.0   19530351\n37             Pacific      Oregon      11139.0          3337.0    4181886\n43  West South Central       Texas      19199.0          6111.0   28628666\n47             Pacific  Washington      16424.0          5880.0    7523869\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n5   Mountain    Colorado       7607.0          3250.0    5691287\n12  Mountain       Idaho       1297.0           715.0    1750536\n26  Mountain     Montana        983.0           422.0    1060665\n28  Mountain      Nevada       7058.0           486.0    3027341\n31  Mountain  New Mexico       1949.0           602.0    2092741\n44  Mountain        Utah       1904.0           972.0    3153550\n50  Mountain     Wyoming        434.0           205.0     577601\n    region   state  individuals  family_members  state_pop\n1  Pacific  Alaska       1434.0           582.0     735139\n            region                 state  ...  family_members  state_pop\n7   South Atlantic              Delaware  ...           374.0     965479\n8   South Atlantic  District of Columbia  ...          3134.0     701547\n9   South Atlantic               Florida  ...          9587.0   21244317\n10  South Atlantic               Georgia  ...          2556.0   10511131\n20  South Atlantic              Maryland  ...          2230.0    6035802\n30    Mid-Atlantic            New Jersey  ...          3350.0    8886025\n32    Mid-Atlantic              New York  ...         52070.0   19530351\n33  South Atlantic        North Carolina  ...          2817.0   10381615\n38    Mid-Atlantic          Pennsylvania  ...          5349.0   12800922\n40  South Atlantic        South Carolina  ...           851.0    5084156\n46  South Atlantic              Virginia  ...          2047.0    8501286\n48  South Atlantic         West Virginia  ...           222.0    1804291\n\n[12 rows x 5 columns]\n      region       state  individuals  family_members  state_pop\n2   Mountain     Arizona       7259.0          2606.0    7158024\n4    Pacific  California     109008.0         20964.0   39461588\n28  Mountain      Nevada       7058.0           486.0    3027341\n44  Mountain        Utah       1904.0           972.0    3153550\n"}],"execution_count":11},{"source":"**New Columns**\n\nCreating new columns from existing columns in the dataframe. It can be called transforming dataframes, mutating a dataframe, and FEATURE ENGINEERING.","metadata":{},"cell_type":"markdown","id":"1fcd53c2-0b33-4785-8347-9f2e5deae7c0"},{"source":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Add total col as sum of individuals and family_members\nhomelessness['total'] = homelessness['individuals'] + homelessness['family_members']\n\n# Add p_individuals col as proportion of total that are individuals\nhomelessness['p_individuals'] = homelessness['individuals'] / homelessness['total']\n\n# See the result\nprint(homelessness)","metadata":{"executionCancelledAt":null,"executionTime":33,"lastExecutedAt":1705680655798,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Add total col as sum of individuals and family_members\nhomelessness['total'] = homelessness['individuals'] + homelessness['family_members']\n\n# Add p_individuals col as proportion of total that are individuals\nhomelessness['p_individuals'] = homelessness['individuals'] / homelessness['total']\n\n# See the result\nprint(homelessness)","outputsMetadata":{"0":{"height":570,"type":"stream"}}},"cell_type":"code","id":"abcb722e-85e4-4277-b5a2-a5feab695802","outputs":[{"output_type":"stream","name":"stdout","text":"                region                 state  ...     total  p_individuals\n0   East South Central               Alabama  ...    3434.0       0.748398\n1              Pacific                Alaska  ...    2016.0       0.711310\n2             Mountain               Arizona  ...    9865.0       0.735834\n3   West South Central              Arkansas  ...    2712.0       0.840708\n4              Pacific            California  ...  129972.0       0.838704\n5             Mountain              Colorado  ...   10857.0       0.700654\n6          New England           Connecticut  ...    3976.0       0.573441\n7       South Atlantic              Delaware  ...    1082.0       0.654344\n8       South Atlantic  District of Columbia  ...    6904.0       0.546060\n9       South Atlantic               Florida  ...   31030.0       0.691041\n10      South Atlantic               Georgia  ...    9499.0       0.730919\n11             Pacific                Hawaii  ...    6530.0       0.632619\n12            Mountain                 Idaho  ...    2012.0       0.644632\n13  East North Central              Illinois  ...   10643.0       0.634408\n14  East North Central               Indiana  ...    5258.0       0.718144\n15  West North Central                  Iowa  ...    2749.0       0.622408\n16  West North Central                Kansas  ...    2216.0       0.651173\n17  East South Central              Kentucky  ...    3688.0       0.741594\n18  West South Central             Louisiana  ...    3059.0       0.830337\n19         New England                 Maine  ...    2516.0       0.576312\n20      South Atlantic              Maryland  ...    7144.0       0.687850\n21         New England         Massachusetts  ...   20068.0       0.339396\n22  East North Central              Michigan  ...    8351.0       0.623758\n23  West North Central             Minnesota  ...    7243.0       0.551291\n24  East South Central           Mississippi  ...    1352.0       0.757396\n25  West North Central              Missouri  ...    5883.0       0.641849\n26            Mountain               Montana  ...    1405.0       0.699644\n27  West North Central              Nebraska  ...    2421.0       0.720777\n28            Mountain                Nevada  ...    7544.0       0.935578\n29         New England         New Hampshire  ...    1450.0       0.575862\n30        Mid-Atlantic            New Jersey  ...    9398.0       0.643541\n31            Mountain            New Mexico  ...    2551.0       0.764014\n32        Mid-Atlantic              New York  ...   91897.0       0.433387\n33      South Atlantic        North Carolina  ...    9268.0       0.696051\n34  West North Central          North Dakota  ...     542.0       0.861624\n35  East North Central                  Ohio  ...   10249.0       0.676066\n36  West South Central              Oklahoma  ...    3871.0       0.729269\n37             Pacific                Oregon  ...   14476.0       0.769481\n38        Mid-Atlantic          Pennsylvania  ...   13512.0       0.604130\n39         New England          Rhode Island  ...    1101.0       0.678474\n40      South Atlantic        South Carolina  ...    3933.0       0.783626\n41  West North Central          South Dakota  ...    1159.0       0.721311\n42  East South Central             Tennessee  ...    7883.0       0.778764\n43  West South Central                 Texas  ...   25310.0       0.758554\n44            Mountain                  Utah  ...    2876.0       0.662031\n45         New England               Vermont  ...    1291.0       0.604183\n46      South Atlantic              Virginia  ...    5975.0       0.657406\n47             Pacific            Washington  ...   22304.0       0.736370\n48      South Atlantic         West Virginia  ...    1243.0       0.821400\n49  East North Central             Wisconsin  ...    4907.0       0.558386\n50            Mountain               Wyoming  ...     639.0       0.679186\n\n[51 rows x 7 columns]\n"}],"execution_count":12},{"source":"# In this exercise, you'll answer the question, \"Which state has the highest number of homeless individuals per 10,000 people in the state?\" Combine your new pandas skills to find out.\n\nimport pandas as pd\nhomelessness = pd.read_csv('datasets/homelessness.csv')\n\n# Create indiv_per_10k col as homeless individuals per 10k state pop\nhomelessness[\"indiv_per_10k\"] = 10000 * homelessness['individuals'] / homelessness['state_pop'] \n\n# Subset rows for indiv_per_10k greater than 20\nhigh_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n\n# Sort high_homelessness by descending indiv_per_10k\nhigh_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\",ascending = False)\n\n# From high_homelessness_srt, select the state and indiv_per_10k cols\nresult = high_homelessness_srt[['state', 'indiv_per_10k']]\n\n# See the result\nprint(result)","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":177,"type":"stream"}}},"cell_type":"code","id":"95ea5ccc-0762-46ff-a65f-a21050c01b13","outputs":[{"output_type":"stream","name":"stdout","text":"                   state  indiv_per_10k\n8   District of Columbia      53.738381\n11                Hawaii      29.079406\n4             California      27.623825\n37                Oregon      26.636307\n28                Nevada      23.314189\n47            Washington      21.829195\n32              New York      20.392363\n"}],"execution_count":13},{"source":"**CHAPTER 2: Summary Statistics**\nSummary statistics are exactly what they sound like - they summarize many numbers in one statistic. \n\nAggregating Data\nYou can call these methods on columns in a dataframe\nmean(), median(), mode(), max(), min(), var(), std(), sum()\n\nCUmmulative Statistics\n1. cumsum() (returns cummulative sum of each row and adds up as it goes down the column)\n2. cummin()\n3. cummax()\n4. cumprod()\n\nCompute Custom Summary Statistics\n1. Use the .agg() function: By defining a function.\n2. The agg() function can accept multiple columns when called and multiple function names.","metadata":{},"cell_type":"markdown","id":"f680bbf1-e816-479a-91ba-de83f00eab39"},{"source":"# Using the Walmart database\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Print the head of the sales DataFrame\nprint(sales.head())\n\n# Print the info about the sales DataFrame\nprint(sales.info())\n\n# Print the mean of weekly_sales\nprint(sales['weekly_sales'].mean())\n\n# Print the median of weekly_sales\nprint(sales['weekly_sales'].median())\n\n# Working with dates\n# Print the maximum of the date column\nprint(sales['date'].max())\n\n# Print the minimum of the date column\nprint(sales['date'].min())","metadata":{"executionCancelledAt":null,"executionTime":40,"lastExecutedAt":1705690794886,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Using the Walmart database\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Print the head of the sales DataFrame\nprint(sales.head())\n\n# Print the info about the sales DataFrame\nprint(sales.info())\n\n# Print the mean of weekly_sales\nprint(sales['weekly_sales'].mean())\n\n# Print the median of weekly_sales\nprint(sales['weekly_sales'].median())","outputsMetadata":{"0":{"height":557,"type":"stream"}}},"cell_type":"code","id":"9db867d1-0c1c-4831-9913-520ef1c71b01","outputs":[{"output_type":"stream","name":"stdout","text":"   store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0      1    A           1  ...      5.727778              0.679451         8.106\n1      1    A           1  ...      8.055556              0.693452         8.106\n2      1    A           1  ...     16.816667              0.718284         7.808\n3      1    A           1  ...     22.527778              0.748928         7.808\n4      1    A           1  ...     27.050000              0.714586         7.808\n\n[5 rows x 9 columns]\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10774 entries, 0 to 10773\nData columns (total 9 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   store                 10774 non-null  int64  \n 1   type                  10774 non-null  object \n 2   department            10774 non-null  int64  \n 3   date                  10774 non-null  object \n 4   weekly_sales          10774 non-null  float64\n 5   is_holiday            10774 non-null  bool   \n 6   temperature_c         10774 non-null  float64\n 7   fuel_price_usd_per_l  10774 non-null  float64\n 8   unemployment          10774 non-null  float64\ndtypes: bool(1), float64(4), int64(2), object(2)\nmemory usage: 684.0+ KB\nNone\n23843.95014850566\n12049.064999999999\n"}],"execution_count":2},{"source":"# Using the .agg() function\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n    \n# Print IQR of the temperature_c column\nprint(sales['temperature_c'].agg(iqr))\n\n# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n\n# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", 'fuel_price_usd_per_l', 'unemployment']].agg(iqr))\n\n# Import NumPy and create custom IQR function\nimport numpy as np\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n\n# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n\nprint(np.median(sales['temperature_c']))","metadata":{"executionCancelledAt":null,"executionTime":44,"lastExecutedAt":1705691578792,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Using the .agg() function\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n    \n# Print IQR of the temperature_c column\nprint(sales['temperature_c'].agg(iqr))\n\n# A custom IQR function\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n\n# Update to print IQR of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", 'fuel_price_usd_per_l', 'unemployment']].agg(iqr))\n\n# Import NumPy and create custom IQR function\nimport numpy as np\ndef iqr(column):\n    return column.quantile(0.75) - column.quantile(0.25)\n\n# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\nprint(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))\n\nprint(np.median(sales['temperature_c']))","outputsMetadata":{"0":{"height":197,"type":"stream"}}},"cell_type":"code","id":"d40ca645-6744-47af-823f-6635707584dd","outputs":[{"output_type":"stream","name":"stdout","text":"16.583333333333336\ntemperature_c           16.583333\nfuel_price_usd_per_l     0.073176\nunemployment             0.565000\ndtype: float64\n        temperature_c  fuel_price_usd_per_l  unemployment\niqr         16.583333              0.073176         0.565\nmedian      16.966667              0.743381         8.099\n16.966666666666665\n"}],"execution_count":8},{"source":"# Getting the sales_1_1 dataframe from sales which contains the department 1 of store 1\n\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\nsales_1_1 = sales[(sales['department'] == 1) & (sales['store'] == 1)]\n# print(sales_1_1)\n\n# Sort sales_1_1 by date\nsales_1_1 = sales_1_1.sort_values('date')\n\n# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\nsales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n\n# Get the cumulative max of weekly_sales, add as cum_max_sales col\nsales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n\n# See the columns you calculated\nprint(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])","metadata":{"executionCancelledAt":null,"executionTime":34,"lastExecutedAt":1705692986853,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Getting the sales_1_1 dataframe from sales which contains the department 1 of store 1\n\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\nsales_1_1 = sales[(sales['department'] == 1) & (sales['store'] == 1)]\n# print(sales_1_1)\n\n# Sort sales_1_1 by date\nsales_1_1 = sales_1_1.sort_values('date')\n\n# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\nsales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n\n# Get the cumulative max of weekly_sales, add as cum_max_sales col\nsales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n\n# See the columns you calculated\nprint(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])","outputsMetadata":{"0":{"height":277,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"acd42844-5938-4c06-b2cf-5e959daab034","outputs":[{"output_type":"stream","name":"stdout","text":"          date  weekly_sales  cum_weekly_sales  cum_max_sales\n0   2010-02-05      24924.50          24924.50       24924.50\n1   2010-03-05      21827.90          46752.40       24924.50\n2   2010-04-02      57258.43         104010.83       57258.43\n3   2010-05-07      17413.94         121424.77       57258.43\n4   2010-06-04      17558.09         138982.86       57258.43\n5   2010-07-02      16333.14         155316.00       57258.43\n6   2010-08-06      17508.41         172824.41       57258.43\n7   2010-09-03      16241.78         189066.19       57258.43\n8   2010-10-01      20094.19         209160.38       57258.43\n9   2010-11-05      34238.88         243399.26       57258.43\n10  2010-12-03      22517.56         265916.82       57258.43\n11  2011-01-07      15984.24         281901.06       57258.43\n"}],"execution_count":14},{"source":"**Counting**\n\nSummarizing categorical data using counting.\n1. Dropping duplicates using the .drop_duplicates(subset = 'column_name') function. It takes an argument SUBSET which is the column we want to find the duplicates in.\n2. Dropping Duplicate Pairs .drop_duplicates(subset = ['column1', 'column2'])\n3. Counting the values of a column df['column'].value_counts(sort = True) or .value_counts(normalize = True) to get the proportion to the total.","metadata":{},"cell_type":"markdown","id":"21c3a930-071c-42e1-86aa-ad85ddf64b44"},{"source":"import pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset = ['store', 'type'])\nprint(store_types.head())\n\n# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset = ['store', 'department'])\nprint(store_depts.head())\n\n# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = sales[sales['is_holiday'] == True].drop_duplicates(subset = 'date')\n\n# Print date col of holiday_dates\nprint(holiday_dates['date'])\n\n# Count the number of stores of each type\nstore_counts = store_types['type'].value_counts()\nprint(store_counts)\n\n# Get the proportion of stores of each type\nstore_props = store_types['type'].value_counts(normalize = True)\nprint(store_props)\n\n# Count the number of each department number and sort\ndept_counts_sorted = store_depts['department'].value_counts(sort = True)\nprint(dept_counts_sorted)\n\n# Get the proportion of departments of each number and sort\ndept_props_sorted = store_depts['department'].value_counts(sort=True, normalize=True)\nprint(dept_props_sorted)","metadata":{"executionCancelledAt":null,"executionTime":59,"lastExecutedAt":1705695570658,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Drop duplicate store/type combinations\nstore_types = sales.drop_duplicates(subset = ['store', 'type'])\nprint(store_types.head())\n\n# Drop duplicate store/department combinations\nstore_depts = sales.drop_duplicates(subset = ['store', 'department'])\nprint(store_depts.head())\n\n# Subset the rows where is_holiday is True and drop duplicate dates\nholiday_dates = sales[sales['is_holiday'] == True].drop_duplicates(subset = 'date')\n\n# Print date col of holiday_dates\nprint(holiday_dates['date'])\n\n# Count the number of stores of each type\nstore_counts = store_types['type'].value_counts()\nprint(store_counts)\n\n# Get the proportion of stores of each type\nstore_props = store_types['type'].value_counts(normalize = True)\nprint(store_props)\n\n# Count the number of each department number and sort\ndept_counts_sorted = store_depts['department'].value_counts(sort = True)\nprint(dept_counts_sorted)\n\n# Get the proportion of departments of each number and sort\ndept_props_sorted = store_depts['department'].value_counts(sort=True, normalize=True)\nprint(dept_props_sorted)","outputsMetadata":{"0":{"height":570,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"fff63b40-f61c-4cc6-9af4-35e765a2e69f","outputs":[{"output_type":"stream","name":"stdout","text":"      store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0         1    A           1  ...      5.727778              0.679451         8.106\n901       2    A           1  ...      4.550000              0.679451         8.324\n1798      4    A           1  ...      6.533333              0.686319         8.623\n2699      6    A           1  ...      4.683333              0.679451         7.259\n3593     10    B           1  ...     12.411111              0.782478         9.765\n\n[5 rows x 9 columns]\n    store type  department  ... temperature_c  fuel_price_usd_per_l  unemployment\n0       1    A           1  ...      5.727778              0.679451         8.106\n12      1    A           2  ...      5.727778              0.679451         8.106\n24      1    A           3  ...      5.727778              0.679451         8.106\n36      1    A           4  ...      5.727778              0.679451         8.106\n48      1    A           5  ...      5.727778              0.679451         8.106\n\n[5 rows x 9 columns]\n498     2010-09-10\n691     2011-11-25\n2315    2010-02-12\n6735    2012-09-07\n6810    2010-12-31\n6815    2012-02-10\n6820    2011-09-09\nName: date, dtype: object\nA    11\nB     1\nName: type, dtype: int64\nA    0.916667\nB    0.083333\nName: type, dtype: float64\n1     12\n55    12\n72    12\n71    12\n67    12\n      ..\n37    10\n48     8\n50     6\n39     4\n43     2\nName: department, Length: 80, dtype: int64\n1     0.012917\n55    0.012917\n72    0.012917\n71    0.012917\n67    0.012917\n        ...   \n37    0.010764\n48    0.008611\n50    0.006459\n39    0.004306\n43    0.002153\nName: department, Length: 80, dtype: float64\n"}],"execution_count":16},{"source":"**Grouped Summary Statistics**\n\n1. Using the .groupby() function\n2. Find multiple columns using the .agg() function.\n3. Group by multiple columns and aggregate by 1 column.\n4. Group by multiple columns and aggregate by multiple columns.","metadata":{},"cell_type":"markdown","id":"86a3cf09-fb71-468e-a87d-c626fa663ffe"},{"source":"# While .groupby() is useful, you can calculate grouped summary statistics without it.\n\n# Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this exercise, you'll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart's total sales were made at each type.\n\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Calc total weekly sales\nsales_all = sales[\"weekly_sales\"].sum()\n\n# Subset for type A stores, calc total weekly sales\nsales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n\n# Subset for type B stores, calc total weekly sales\nsales_B = sales[sales['type'] == 'B']['weekly_sales'].sum()\n\n# Subset for type C stores, calc total weekly sales\nsales_C = sales[sales['type'] == 'C']['weekly_sales'].sum()\n\n# Get proportion for each type\nsales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\nprint(sales_propn_by_type)","metadata":{"executionCancelledAt":null,"executionTime":148,"lastExecutedAt":1705933867034,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# While .groupby() is useful, you can calculate grouped summary statistics without it.\n\n# Walmart distinguishes three types of stores: \"supercenters,\" \"discount stores,\" and \"neighborhood markets,\" encoded in this dataset as type \"A,\" \"B,\" and \"C.\" In this exercise, you'll calculate the total sales made at each store type, without using .groupby(). You can then use these numbers to see what proportion of Walmart's total sales were made at each type.\n\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Calc total weekly sales\nsales_all = sales[\"weekly_sales\"].sum()\n\n# Subset for type A stores, calc total weekly sales\nsales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n\n# Subset for type B stores, calc total weekly sales\nsales_B = sales[sales['type'] == 'B']['weekly_sales'].sum()\n\n# Subset for type C stores, calc total weekly sales\nsales_C = sales[sales['type'] == 'C']['weekly_sales'].sum()\n\n# Get proportion for each type\nsales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\nprint(sales_propn_by_type)"},"cell_type":"code","id":"e8e99692-ea9c-4c92-a250-8125b1b1fcbc","outputs":[{"output_type":"stream","name":"stdout","text":"[0.9097747 0.0902253 0.       ]\n"}],"execution_count":1},{"source":"# The .groupby() method makes life much easier. In this exercise, you'll perform the same calculations as last time, except you'll use the .groupby() method. You'll also perform calculations on data grouped by two variables to see if sales differ by store type depending on if it's a holiday week or not.\n\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Group by type; calc total weekly sales\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Calculate the proportion of sales at each store type by dividing by the sum of sales_by_type. Assign to sales_propn_by_type.\n# Get proportion for each type\nsales_propn_by_type = sales_by_type / sum(sales_by_type)\nprint(sales_propn_by_type)\n\n# From previous step\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Group by type and is_holiday; calc total weekly sales\nsales_by_type_is_holiday = sales.groupby(['type', 'is_holiday'])['weekly_sales'].sum()\nprint(sales_by_type_is_holiday)","metadata":{"executionCancelledAt":null,"executionTime":35,"lastExecutedAt":1705933975233,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# The .groupby() method makes life much easier. In this exercise, you'll perform the same calculations as last time, except you'll use the .groupby() method. You'll also perform calculations on data grouped by two variables to see if sales differ by store type depending on if it's a holiday week or not.\n\nimport pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Group by type; calc total weekly sales\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Calculate the proportion of sales at each store type by dividing by the sum of sales_by_type. Assign to sales_propn_by_type.\n# Get proportion for each type\nsales_propn_by_type = sales_by_type / sum(sales_by_type)\nprint(sales_propn_by_type)\n\n# From previous step\nsales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n\n# Group by type and is_holiday; calc total weekly sales\nsales_by_type_is_holiday = sales.groupby(['type', 'is_holiday'])['weekly_sales'].sum()\nprint(sales_by_type_is_holiday)"},"cell_type":"code","id":"88afcd2a-d225-4e85-83a6-f7093a8ca94b","outputs":[{"output_type":"stream","name":"stdout","text":"type\nA    0.909775\nB    0.090225\nName: weekly_sales, dtype: float64\ntype  is_holiday\nA     False         2.336927e+08\n      True          2.360181e+04\nB     False         2.317678e+07\n      True          1.621410e+03\nName: weekly_sales, dtype: float64\n"}],"execution_count":2},{"source":"import pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Import numpy with the alias np\nimport numpy as np\n\n# Get the min, max, mean, and median of weekly_sales for each store type using .groupby() and .agg(). Store this as sales_stats. Make sure to use numpy functions!\n# Get the min, max, mean, and median of unemployment and fuel_price_usd_per_l for each store type. Store this as unemp_fuel_stats.\n\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\nsales_stats = sales.groupby('type')['weekly_sales'].agg([np.min,np.max,np.mean,np.median])\n\n# Print sales_stats\nprint(sales_stats)\n\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\nunemp_fuel_stats = sales.groupby('type')['unemployment', 'fuel_price_usd_per_l'].agg([np.min,np.max,np.mean,np.median])\n\n# Print unemp_fuel_stats\nprint(unemp_fuel_stats)","metadata":{"executionCancelledAt":null,"executionTime":1077,"lastExecutedAt":1705700117570,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nsales = pd.read_csv('datasets/walmart.csv')\n\n# Import numpy with the alias np\nimport numpy as np\n\n# Get the min, max, mean, and median of weekly_sales for each store type using .groupby() and .agg(). Store this as sales_stats. Make sure to use numpy functions!\n# Get the min, max, mean, and median of unemployment and fuel_price_usd_per_l for each store type. Store this as unemp_fuel_stats.\n\n# For each store type, aggregate weekly_sales: get min, max, mean, and median\nsales_stats = sales.groupby('type')['weekly_sales'].agg([np.min,np.max,np.mean,np.median])\n\n# Print sales_stats\nprint(sales_stats)\n\n# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\nunemp_fuel_stats = sales.groupby('type')['unemployment', 'fuel_price_usd_per_l'].agg([np.min,np.max,np.mean,np.median])\n\n# Print unemp_fuel_stats\nprint(unemp_fuel_stats)","outputsMetadata":{"0":{"height":237,"type":"stream"}}},"cell_type":"code","id":"1955e62b-0684-482e-ba9f-8ebc05751f02","outputs":[{"output_type":"stream","name":"stdout","text":"        amin       amax          mean    median\ntype                                           \nA    -1098.0  293966.05  23674.667242  11943.92\nB     -798.0  232558.51  25696.678370  13336.08\n     unemployment                   ... fuel_price_usd_per_l                    \n             amin   amax      mean  ...                 amax      mean    median\ntype                                ...                                         \nA           3.879  8.992  7.972611  ...             1.107410  0.744619  0.735455\nB           7.170  9.765  9.279323  ...             1.107674  0.805858  0.803348\n\n[2 rows x 8 columns]\n"}],"execution_count":1},{"source":"**Pivot Tables**\nANother way of grouping in Python.","metadata":{},"cell_type":"markdown","id":"4027262b-233b-4753-86d6-2b1f85b9f747"},{"source":"## Explore Datasets\nUse the DataFrames imported in the first cell to explore the data and practice your skills!\n- Print the highest weekly sales for each `department` in the `walmart` DataFrame. Limit your results to the top five departments, in descending order. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/aggregating-dataframes?ex=1).\n- What was the total `nb_sold` of organic avocados in 2017 in the `avocado` DataFrame? If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/slicing-and-indexing-dataframes?ex=6).\n- Create a bar plot of the total number of homeless people by region in the `homelessness` DataFrame. Order the bars in descending order. Bonus: create a horizontal bar chart. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/creating-and-visualizing-dataframes?ex=1).\n- Create a line plot with two lines representing the temperatures in Toronto and Rome. Make sure to properly label your plot. Bonus: add a legend for the two lines. If you're stuck, try reviewing this [video](https://campus.datacamp.com/courses/data-manipulation-with-pandas/creating-and-visualizing-dataframes?ex=1).","metadata":{},"id":"c09c5c3a","cell_type":"markdown"}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}